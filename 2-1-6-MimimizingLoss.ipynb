{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gangasagarhl/tiny_ML/blob/main/2-1-6-MimimizingLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtmQDB6EjReV"
      },
      "outputs": [],
      "source": [
        "# First import the functions we will need\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmVeKNDMQ8Y1"
      },
      "outputs": [],
      "source": [
        "# This script requires TensorFlow 2 and Python 3.\n",
        "if tf.__version__.split('.')[0] != '2':\n",
        "    raise Exception((f\"The script is developed and tested for tensorflow 2. \"\n",
        "                     f\"Current version: {tf.__version__}\"))\n",
        "\n",
        "if sys.version_info.major < 3:\n",
        "    raise Exception((f\"The script is developed and tested for Python 3. \"\n",
        "                     f\"Current version: {sys.version_info.major}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O5-58WYokl3"
      },
      "source": [
        "# GradientTape\n",
        "\n",
        "The Calculus is managed by a TensorFlow Gradient Tape. You can learn more about the gradient tape at https://www.tensorflow.org/api_docs/python/tf/GradientTape, and we will discuss it later in the course."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdhgbGE2j02L"
      },
      "outputs": [],
      "source": [
        "# Define our initial guess\n",
        "INITIAL_W = 10.0\n",
        "INITIAL_B = 10.0\n",
        "\n",
        "# Define our loss function\n",
        "def loss(predicted_y, target_y):\n",
        "  return tf.reduce_mean(tf.square(predicted_y - target_y))\n",
        "\n",
        "# Define our training procedure\n",
        "def train(model, inputs, outputs, learning_rate):\n",
        "  with tf.GradientTape() as t:\n",
        "    current_loss = loss(model(inputs), outputs)\n",
        "    # Here is where you differentiate the model values with respect to the loss function\n",
        "    dw, db = t.gradient(current_loss, [model.w, model.b])\n",
        "    # And here is where you update the model values based on the learning rate chosen\n",
        "    model.w.assign_sub(learning_rate * dw)\n",
        "    model.b.assign_sub(learning_rate * db)\n",
        "    return current_loss\n",
        "\n",
        "# Define our simple linear regression model\n",
        "class Model(object):\n",
        "  def __init__(self):\n",
        "    # Initialize the weights\n",
        "    self.w = tf.Variable(INITIAL_W)\n",
        "    self.b = tf.Variable(INITIAL_B)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.w * x + self.b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX1nG061Q8Y5"
      },
      "source": [
        "### Train our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8akmDCoAjgak"
      },
      "outputs": [],
      "source": [
        "# Define our input data and learning rate\n",
        "xs = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]\n",
        "ys = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]\n",
        "LEARNING_RATE=0.09\n",
        "\n",
        "# Instantiate our model\n",
        "model = Model()\n",
        "\n",
        "# Collect the history of w-values and b-values to plot later\n",
        "list_w, list_b = [], []\n",
        "epochs = range(50)\n",
        "losses = []\n",
        "for epoch in epochs:\n",
        "  list_w.append(model.w.numpy())\n",
        "  list_b.append(model.b.numpy())\n",
        "  current_loss = train(model, xs, ys, learning_rate=LEARNING_RATE)\n",
        "  losses.append(current_loss)\n",
        "  print(f\"Epoch {epoch:2d}: w={list_w[-1]:1.2f} b={list_b[-1]:1.2f}, \"\n",
        "        f\"loss={current_loss:2.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOYTz0MbQ8Y7"
      },
      "source": [
        "### Plot our trained values over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGgb5grSk8Ax"
      },
      "outputs": [],
      "source": [
        "# Plot the w-values and b-values for each training Epoch against the true values\n",
        "TRUE_w = 2.0\n",
        "TRUE_b = -1.0\n",
        "plt.plot(epochs, list_w, 'r', epochs, list_b, 'b')\n",
        "plt.plot([TRUE_w] * len(epochs), 'r--', [TRUE_b] * len(epochs), 'b--')\n",
        "plt.legend(['w', 'b', 'True w', 'True b'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "    y1 = x ** 3\n",
        "    y2 = x ** 5\n",
        "\n",
        "# Compute gradients separately\n",
        "dy1_dx = tape.gradient(y1, x)\n",
        "dy2_dx = tape.gradient(y2, x)\n",
        "\n",
        "print(\"dy1/dx:\", dy1_dx.numpy())  # Output: 6.0\n",
        "print(\"dy2/dx:\", dy2_dx.numpy())  # Output: 27.0\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaAowfLTB3lq",
        "outputId": "cc8f06bc-4d53-4be3-d993-56a50cc6e1fd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy1/dx: 12.0\n",
            "dy2/dx: 80.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a simple input variable\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "# Define a computation with tf.GradientTape\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x ** 2  # Function: y = x^2\n",
        "\n",
        "# Compute the gradient of y with respect to x\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(\"dy/dx:\", dy_dx.numpy())  # Output: 6.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL9w6JWxuhFu",
        "outputId": "a428ab82-19d5-4038-c330-32dcd2f49568"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy/dx: 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x ** 2\n",
        "    z = tf.stop_gradient(y) + 3  # Stop tracking gradients for y\n",
        "\n",
        "# Gradient is computed only for the active path\n",
        "grad = tape.gradient(z, x)\n",
        "print(grad)  # Output: None (y is excluded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if0tI_jq6yY8",
        "outputId": "4ed87e60-df2c-4592-a3e5-033ea339bdab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable(2.0)  # A trainable variable\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = x ** 2             # y = x^2\n",
        "    z = tf.stop_gradient(y) + 3  # Stop gradient flow through y\n",
        "\n",
        "# Compute gradient of z with respect to x\n",
        "grad = tape.gradient(y, x)\n",
        "print(\"Gradient:\", grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbAmiQ-lAfSS",
        "outputId": "0334ca96-0652-4d83-b86e-5acdbbaed0e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient: tf.Tensor(4.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Mimimizing-Loss.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}